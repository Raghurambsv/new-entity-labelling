{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New entities recogniser, annotatation with BILUO scheme, using spaCy\n",
    "\n",
    " * Use of pretrained Machine Learning (ML) model is quite prevalent in vision-related problems, where it is tuned for the desired task, nonetheless, last couple of years ([Peters et al.](https://www.aclweb.org/anthology/N18-1202/), [Akbik et al.](https://alanakbik.github.io/papers/coling2018.pdf)) has spurred the use of pretrained Natural Language Processing (NLP) models to do the same for NLP tasks. \n",
    " \n",
    " * This notebook uses a pretrained [spaCy](https://spacy.io/models/en) model to train for user-specific entities in texts. \n",
    " \n",
    " * Read [here](https://ruder.io/state-of-transfer-learning-in-nlp/) for the latest state of transfer learning in NLP.\n",
    " \n",
    " * The pretrained [model](https://spacy.io/models/en) used here is convolution neural network (CNN) architecture trained on [OneNotes](https://catalog.ldc.upenn.edu/LDC2013T19) \n",
    " \n",
    " * The customised entity recogniser is trained on [BILUO](https://spacy.io/api/annotation#biluo) scheme. Note here that the BILUO scheme trains and performs better than IOB scheme. Read faq of [README](README.md) \n",
    " \n",
    " * This is an extension with explanation for already provided [example](https://github.com/explosion/spaCy/blob/master/examples/training/train_new_entity_type.py) by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en') # or any other specific model like 'en_core_web_md' more at https://spacy.io/models/en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Annotations\n",
    "\n",
    " * [Using BILUO scheme](#biluo)\n",
    " * [Using offset indices](#offset)\n",
    " * [Custom Doc](#customdoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BILUO Scheme\n",
    "<a id='biluo'> </a>\n",
    "\n",
    " * Annotate your data using [BILUO](https://spacy.io/api/annotation#biluo) scheme where each token is from [doc](https://spacy.io/api) created with model\n",
    " ```\n",
    " text = 'Write your text here.'\n",
    " doc = nlp(text)\n",
    " \n",
    " # for token in doc:\n",
    " # write the token in a file for annotating it later \n",
    " ```\n",
    " \n",
    " * An example is provided [here](ner-token-per-line.biluo) \n",
    " \n",
    " * This is less cumbersome process of annotating than offset indices- shown later.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gor reproducing same results during mutiple run\n",
    "s = 999\n",
    "np.random.seed(s)\n",
    "spacy.util.fix_random_seed(s)\n",
    "\n",
    "# if Training with GPU also\n",
    "# cupy.random.seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from file\n",
    "\n",
    "import pandas as pd\n",
    "dpath = 'ner-token-per-line.biluo'\n",
    "df = pd.read_csv(dpath, sep=',')\n",
    "words  = df.word.values\n",
    "ents = df.label.values\n",
    "text = ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all the new annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Existing Entities] =  ['B-ORG', 'B-DATE', 'B-PERSON', 'B-GPE', 'B-MONEY', 'B-CARDINAL', 'B-NORP', 'B-PERCENT', 'B-WORK_OF_ART', 'B-LOC', 'B-TIME', 'B-QUANTITY', 'B-FAC', 'B-EVENT', 'B-ORDINAL', 'B-PRODUCT', 'B-LAW', 'B-LANGUAGE', 'I-ORG', 'I-DATE', 'I-PERSON', 'I-GPE', 'I-MONEY', 'I-CARDINAL', 'I-NORP', 'I-PERCENT', 'I-WORK_OF_ART', 'I-LOC', 'I-TIME', 'I-QUANTITY', 'I-FAC', 'I-EVENT', 'I-ORDINAL', 'I-PRODUCT', 'I-LAW', 'I-LANGUAGE', 'L-ORG', 'L-DATE', 'L-PERSON', 'L-GPE', 'L-MONEY', 'L-CARDINAL', 'L-NORP', 'L-PERCENT', 'L-WORK_OF_ART', 'L-LOC', 'L-TIME', 'L-QUANTITY', 'L-FAC', 'L-EVENT', 'L-ORDINAL', 'L-PRODUCT', 'L-LAW', 'L-LANGUAGE', 'U-ORG', 'U-DATE', 'U-PERSON', 'U-GPE', 'U-MONEY', 'U-CARDINAL', 'U-NORP', 'U-PERCENT', 'U-WORK_OF_ART', 'U-LOC', 'U-TIME', 'U-QUANTITY', 'U-FAC', 'U-EVENT', 'U-ORDINAL', 'U-PRODUCT', 'U-LAW', 'U-LANGUAGE', 'O']\n",
      "\n",
      "\n",
      "[New Entities] =  ['I-DATED', 'B-DATED', 'U-DATED', 'L-DATED']\n"
     ]
    }
   ],
   "source": [
    "add_ents = ['DATED'] # Provide all the extra entity that the model should recognise beyond existing named-entities https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "# Create a pipe if it does not exist\n",
    "# Piplines in pretrained model: tagger, parser, ner create new if blank model is to be trained using `spacy.blank('en')`\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\") # \"architecture\": \"ensemble\" simple_cnn ensemble, bow # https://spacy.io/api/annotation\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "prev_ents = ner.move_names\n",
    "print('[Existing Entities] = ', ner.move_names)\n",
    "\n",
    "for ent in add_ents:\n",
    "    ner.add_label(ent)\n",
    "    \n",
    "new_ents = ner.move_names\n",
    "# print('\\n[All Entities] = ', ner.move_names)\n",
    "\n",
    "print('\\n\\n[New Entities] = ', list(set(new_ents) - set(prev_ents)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create Dataset\n",
    "from spacy.gold import GoldParse\n",
    "doc = nlp.make_doc(text)\n",
    "g = GoldParse(doc, entities=ents)\n",
    "\n",
    "# Add examples as avaialble or needed\n",
    "X = [doc, doc]\n",
    "Y = [g, g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id='training'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OtherPipes] = ['tagger', 'parser'] will be disabled\n"
     ]
    }
   ],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "print(f'[OtherPipes] = {other_pipes} will be disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 15.094052302083471}\n",
      "Losses {'ner': 2.813956273703063}\n",
      "Losses {'ner': 2.6137797097536524}\n",
      "Losses {'ner': 2.6699867776599628}\n",
      "Losses {'ner': 3.6325323196373986}\n",
      "Losses {'ner': 3.418292572110345}\n",
      "Losses {'ner': 2.7101756360972948}\n",
      "Losses {'ner': 2.0605769809223262}\n",
      "Losses {'ner': 1.8786397921994649}\n",
      "Losses {'ner': 1.7351509290678873}\n",
      "Losses {'ner': 1.4021572820718298}\n",
      "Losses {'ner': 4.4508419834298945}\n",
      "Losses {'ner': 11.466295966513655}\n",
      "Losses {'ner': 10.674786345421065}\n",
      "Losses {'ner': 9.8725883235784604}\n",
      "Losses {'ner': 8.5278043256903935}\n",
      "Losses {'ner': 6.6744801108174743}\n",
      "Losses {'ner': 5.7457604113131504}\n",
      "Losses {'ner': 5.4725865582614581}\n",
      "Losses {'ner': 5.2311404237445913}\n"
     ]
    }
   ],
   "source": [
    "model = None # Since we training a fresh model not a saved model\n",
    "n_iter = 20\n",
    "with nlp.disable_pipes(*other_pipes):  # only train ner\n",
    "    # optimizer = nlp.begin_training()\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        nlp.update(X, Y,  sgd=optimizer, drop=0.0, losses=losses)\n",
    "        # nlp.entity.update(d, g)\n",
    "        print(\"Losses\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week. DATED\n"
     ]
    }
   ],
   "source": [
    "test_text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(test_text)\n",
    "# print(\"[Entities] in '%s'\" % test_text, '\\n\\n')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Offset\n",
    "<a id='offset'> </a>\n",
    "\n",
    " * Training process is the same as the previous one except data creation is different.\n",
    " * Here annotations are created using offset indices while the scheme is of course still BILUO.\n",
    " * One can see that this is a bit clumsy to use, of course, still works.\n",
    " * I can not make a claim which is better or has similar performance- as one needs to perform experiments to make any claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one instance\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = text\n",
    "g = {'entities': [(5, 20, 'PERSON'), (61, 67, 'ORG'), (71, 75, 'DATE'), (173, 181, 'NORP'), \n",
    "    (271, 276, 'PERSON'), (299, 305, 'ORG'), (306, 323, 'DATED')]}\n",
    "\n",
    "X = [doc]\n",
    "Y = [g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2.5312684880080742}\n",
      "Losses {'ner': 1.8361174537084519}\n",
      "Losses {'ner': 1.4230114231602224}\n",
      "Losses {'ner': 1.5833287709093908}\n",
      "Losses {'ner': 1.2528726923071076}\n",
      "Losses {'ner': 0.6255715996904212}\n",
      "Losses {'ner': 0.25580679669997786}\n",
      "Losses {'ner': 0.23035309925435654}\n",
      "Losses {'ner': 0.12631427119187083}\n",
      "Losses {'ner': 0.033360898805490091}\n",
      "Losses {'ner': 0.006467470522833807}\n",
      "Losses {'ner': 0.001925140257319134}\n",
      "Losses {'ner': 0.00082710340770614688}\n",
      "Losses {'ner': 0.00037566671037990467}\n",
      "Losses {'ner': 0.00014929488204637952}\n",
      "Losses {'ner': 5.2590473040970581e-05}\n",
      "Losses {'ner': 1.7812523872109617e-05}\n",
      "Losses {'ner': 6.13590462614764e-06}\n",
      "Losses {'ner': 2.2239461826400325e-06}\n",
      "Losses {'ner': 8.7037971907284566e-07}\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATED\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipes):  # only train ner\n",
    "    # optimizer = nlp.begin_training()\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        nlp.update(X, Y,  sgd=optimizer, drop=0.0, losses=losses)\n",
    "        # nlp.entity.update(d, g)\n",
    "        print(\"Losses\", losses)\n",
    "doc = nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Doc\n",
    "<a id='customdoc'> </a>\n",
    "\n",
    " * Typically tokens created by the nlp model splits not only by spaces but also by stop words and special characters. \n",
    " \n",
    " * We might want to label each word (split only with spaces or something else) rather than for each token as generated by NLP model (`doc = nlp(text)`) then in such case we need to modify a bit.\n",
    " \n",
    " * Please note that while we can have custom doc but a token in the doc can not be space.\n",
    " \n",
    " * In my experiments I did not find this helpful, in fact, it was weaker than other two annotation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc # https://spacy.io/api/doc\n",
    "\n",
    "import pandas as pd\n",
    "dpath = 'ner-token-per-line.biluo' # It not necessarily \n",
    "df = pd.read_csv(dpath, sep=',')\n",
    "words  = df.word.values\n",
    "ents = df.label.values\n",
    "text = ' '.join(words)\n",
    "\n",
    "spaces = [True]*len(words)\n",
    "spaces[-1] = False # so remove space in last\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces) # Custom Doc\n",
    "g = GoldParse(doc, entities=ents)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    " * Read this paper by [Akbik et al.](https://alanakbik.github.io/papers/coling2018.pdf) should help in understanding the algorithm behind the sequence labelling i.e. multiple word entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_v1.0]",
   "language": "python",
   "name": "conda-env-torch_v1.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
